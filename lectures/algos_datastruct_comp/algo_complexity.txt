*** Analisi degli algoritmi - Complessità computazionale ***

* Analisi degli algoritmi *
 - Analizzare un algoritmo significa stimare le risorse che esso richiede
 - Esempi di risorse: memoria, banda, hardware... ci concentreremo sulla
   risorsa "tempo"
 - Generalmente, il tempo che impiega un algoritmo dipende dalla dimensione
   dell'input, quindi il "running time" (tempo di esecuzione) di un algoritmo
   viene espresso come funzione della dimensione dell'input
 - Nel problema dell'ordinamento, la dimensione dell'input è il numero di 
   elementi dell'array da ordinare; in molti problemi sui grafi, la dimensione
   è composta da numero di nodi e numero di archi
 - Running time: è il numero di passi eseguiti da un algoritmo su un particolare
   input. Anche per 2 input della stessa dimensione, il running time può
   dipendere dal particolare input

 - Best-case running time: running time più breve di un algoritmo
 - Worst-case running time: è un limite superiore (upper bound)
   al running time per qualsiasi input (riassumibile con la frase
   "peggio di così non può fare")
 - Average-case running time: running time nel caso medio


* Tasso di crescita di un running time *
 - Se l'espressione del running time di un algoritmo è
   a*n^2 + b*n + c, dove n è la dimensione dell'input,
   e a, b, c sono delle costanti, si mantiene soltanto
   il termine "dominante", senza coefficiente, arrivando
   all'espressione: O(n^2) (big O di n^2)
 - Intuitivamente, un algoritmo che ha worst-case running time
   O(n^2) è più lento (su input di dimensioni significative)
   di un algoritmo che ha worst case O(n*lg(n)), ma è più
   veloce di un algoritmo con running time O(n^3)
 - Esempi: Merge sort (complessità worst-case O(n*lg(n))), 
   su input di dimensioni significative, "batte" Insertion
   sort (complessità worst-case O(n^2))
 - Quando si analizza il running time di un algoritmo per input
   grandi, si sta valutando la sua efficienza "asintotica", ossia
   come il tempo di esecuzione cresce al limite, ossia per input
   di dimensioni tendenti all'infinito


* Big O notation *
 - O(g(n)) = {f(n): esiste una costante positiva c e un n_0 t.c.
              0 <= f(n) <= c * g(n), per ogni n >= n_0}
 - Per una comprensione intuitiva:
    https://en.wikipedia.org/wiki/Big_O_notation#/media/File:Big-O-notation.png


* Complessità computazionale (worst-case) di alcune operazioni-algoritmi * 
  - O(1) (costante): accesso ad un elemento di un array
  - O(lg(n)) (logaritmica): binary search, operazioni comuni su alberi
    bilanciati
  - O(n) (lineare): ricerca lineare, Counting sort
  - O(n*lg(n)) (linearitmica): Merge sort, Heapsort (si può dimostrare che
    l'ordinamento basato su confronti non può battere O(n*lg(n))).
    Molto intuitivamente: quando nell'analisi di un algoritmo "salta fuori" 
    un albero, nell'espressione della sua complessità "salterà fuori" un
    logaritmo
  - O(n^2) (quadratica): Insertion sort, Bubble sort, Selection sort,
    Quicksort.
    NB: Quicksort, nel caso medio, ha complessità O(n*lg(n)).
    Molto intuitivamente: quando ci sono 2 cicli annidati, generalmente
    si arriva a O(n^2)
  - O(n^3) (cubica): moltiplicazione tra matrici naive, eliminazione gaussiana.
    Molto intuitivamente: quando ci sono 3 cicli annidati, generalmente si
    arriva a O(n^3)
  - O(c^n), per c > 1 (esponenziale): diversi problemi "intrattabili"


* Esempio pratico *
  - https://github.com/Cyofanni/high-school-cs-class/blob/main/algorithm_drills/sorting_complexity_experiment.py